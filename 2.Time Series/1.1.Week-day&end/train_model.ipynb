{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = np.load('train_feature.npy')\n",
    "train_label = np.load('train_label.npy')\n",
    "test_feature = np.load('test_feature.npy')\n",
    "test_label = np.load('test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "learning_rate = 0.01\n",
    "EPOCH = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = torch.from_numpy(train_feature)\n",
    "train_feature = train_feature.float()\n",
    "train_feature = Variable(train_feature)\n",
    "train_feature = train_feature.cuda()\n",
    "\n",
    "train_label = torch.from_numpy(train_label)\n",
    "train_label = train_label.float()\n",
    "train_label = Variable(train_label)\n",
    "train_label = train_label.cuda()\n",
    "\n",
    "test_feature = torch.from_numpy(test_feature)\n",
    "test_feature = test_feature.float()\n",
    "test_feature = Variable(test_feature)\n",
    "test_feature = test_feature.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRG(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n",
    "        super(LSTMRG, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.reg = nn.Linear(hidden_size, output_size)  # 拼接全连接层\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        s, b, h = x.shape  # (seq, batch, hidden_size)\n",
    "        x = x.view(s * b, h)\n",
    "        x = self.reg(x)\n",
    "        x = x.view(s, b, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 600, train_loss: 0.0010536, test_loss: 0.0014383\n",
      "\tTrain Epoch: 600, train_loss: 0.0020553, test_loss: 0.0039976\n",
      "\tTrain Epoch: 600, train_loss: 0.0003715, test_loss: 0.0005594\n",
      "\tTrain Epoch: 600, train_loss: 0.0014555, test_loss: 0.0024732\n",
      "\tTrain Epoch: 600, train_loss: 0.0010827, test_loss: 0.0015273\n",
      "\tTrain Epoch: 600, train_loss: 0.0005349, test_loss: 0.0006747\n",
      "\tTrain Epoch: 600, train_loss: 0.0006395, test_loss: 0.0007507\n",
      "\tTrain Epoch: 600, train_loss: 0.0014343, test_loss: 0.0027720\n",
      "\tTrain Epoch: 600, train_loss: 0.0008226, test_loss: 0.0068344\n",
      "\tTrain Epoch: 600, train_loss: 0.0003835, test_loss: 0.0003890\n",
      "\tTrain Epoch: 600, train_loss: 0.0016433, test_loss: 0.0028040\n",
      "\tTrain Epoch: 600, train_loss: 0.0004031, test_loss: 0.0004841\n",
      "\tTrain Epoch: 600, train_loss: 0.0015404, test_loss: 0.0035509\n",
      "\tTrain Epoch: 600, train_loss: 0.0029878, test_loss: 0.0078303\n",
      "\tTrain Epoch: 600, train_loss: 0.0034660, test_loss: 0.0044373\n",
      "\tTrain Epoch: 600, train_loss: 0.0012569, test_loss: 0.0020670\n",
      "\tTrain Epoch: 600, train_loss: 0.0009602, test_loss: 0.0032174\n",
      "\tTrain Epoch: 600, train_loss: 0.0007675, test_loss: 0.0023767\n",
      "\tTrain Epoch: 600, train_loss: 0.0009116, test_loss: 0.0039884\n",
      "\tTrain Epoch: 600, train_loss: 0.0020844, test_loss: 0.0077331\n",
      "\tTrain Epoch: 600, train_loss: 0.0010332, test_loss: 0.0021820\n",
      "\tTrain Epoch: 600, train_loss: 0.0007518, test_loss: 0.0011592\n",
      "\tTrain Epoch: 600, train_loss: 0.0016961, test_loss: 0.0026912\n",
      "\tTrain Epoch: 600, train_loss: 0.0008748, test_loss: 0.0017619\n",
      "\tTrain Epoch: 600, train_loss: 0.0008177, test_loss: 0.0017901\n",
      "\tTrain Epoch: 600, train_loss: 0.0006998, test_loss: 0.0007857\n",
      "\tTrain Epoch: 600, train_loss: 0.0007236, test_loss: 0.0008214\n",
      "\tTrain Epoch: 600, train_loss: 0.0012809, test_loss: 0.0012810\n",
      "\tTrain Epoch: 600, train_loss: 0.0003954, test_loss: 0.0005276\n",
      "\tTrain Epoch: 600, train_loss: 0.0012767, test_loss: 0.0016741\n",
      "\tTrain Epoch: 600, train_loss: 0.0035030, test_loss: 0.0035877\n",
      "\tTrain Epoch: 600, train_loss: 0.0045332, test_loss: 0.0061403\n",
      "\tTrain Epoch: 600, train_loss: 0.0003947, test_loss: 0.0006258\n",
      "\tTrain Epoch: 600, train_loss: 0.0006067, test_loss: 0.0009212\n",
      "\tTrain Epoch: 600, train_loss: 0.0026897, test_loss: 0.0177867\n",
      "\tTrain Epoch: 600, train_loss: 0.0007314, test_loss: 0.0019443\n",
      "\tTrain Epoch: 600, train_loss: 0.0006535, test_loss: 0.0008729\n",
      "\tTrain Epoch: 600, train_loss: 0.0005373, test_loss: 0.0007244\n",
      "\tTrain Epoch: 600, train_loss: 0.0013132, test_loss: 0.0018112\n",
      "\tTrain Epoch: 600, train_loss: 0.0008449, test_loss: 0.0011899\n",
      "\tTrain Epoch: 600, train_loss: 0.0004307, test_loss: 0.0007144\n",
      "\tTrain Epoch: 600, train_loss: 0.0016557, test_loss: 0.0048575\n",
      "\tTrain Epoch: 600, train_loss: 0.0005241, test_loss: 0.0007431\n",
      "\tTrain Epoch: 600, train_loss: 0.0019046, test_loss: 0.0033433\n",
      "\tTrain Epoch: 600, train_loss: 0.0005629, test_loss: 0.0008099\n",
      "\tTrain Epoch: 600, train_loss: 0.0022489, test_loss: 0.0030859\n",
      "\tTrain Epoch: 600, train_loss: 0.0007923, test_loss: 0.0027564\n",
      "\tTrain Epoch: 600, train_loss: 0.0021459, test_loss: 0.0040539\n",
      "\tTrain Epoch: 600, train_loss: 0.0019771, test_loss: 0.0052623\n",
      "\tTrain Epoch: 600, train_loss: 0.0029863, test_loss: 0.0062334\n",
      "\tTrain Epoch: 600, train_loss: 0.0023388, test_loss: 0.0058266\n",
      "\tTrain Epoch: 600, train_loss: 0.0015757, test_loss: 0.0030609\n",
      "\tTrain Epoch: 600, train_loss: 0.0014126, test_loss: 0.0015054\n",
      "\tTrain Epoch: 600, train_loss: 0.0026115, test_loss: 0.0056128\n",
      "\tTrain Epoch: 600, train_loss: 0.0004521, test_loss: 0.0007116\n",
      "\tTrain Epoch: 600, train_loss: 0.0021007, test_loss: 0.0039084\n",
      "\tTrain Epoch: 600, train_loss: 0.0012553, test_loss: 0.0016813\n",
      "\tTrain Epoch: 600, train_loss: 0.0018817, test_loss: 0.0037275\n",
      "\tTrain Epoch: 600, train_loss: 0.0008487, test_loss: 0.0018537\n",
      "\tTrain Epoch: 600, train_loss: 0.0016575, test_loss: 0.0059333\n",
      "\tTrain Epoch: 600, train_loss: 0.0010487, test_loss: 0.0024473\n",
      "\tTrain Epoch: 600, train_loss: 0.0020655, test_loss: 0.0050841\n",
      "\tTrain Epoch: 600, train_loss: 0.0013559, test_loss: 0.0019759\n",
      "\tTrain Epoch: 600, train_loss: 0.0009857, test_loss: 0.0037897\n",
      "\tTrain Epoch: 600, train_loss: 0.0006405, test_loss: 0.0012915\n",
      "\tTrain Epoch: 600, train_loss: 0.0013289, test_loss: 0.0027433\n",
      "\tTrain Epoch: 600, train_loss: 0.0006651, test_loss: 0.0009069\n",
      "\tTrain Epoch: 600, train_loss: 0.0020403, test_loss: 0.0038670\n",
      "\tTrain Epoch: 600, train_loss: 0.0007723, test_loss: 0.0008977\n",
      "\tTrain Epoch: 600, train_loss: 0.0022661, test_loss: 0.0033294\n",
      "\tTrain Epoch: 600, train_loss: 0.0005499, test_loss: 0.0008642\n",
      "\tTrain Epoch: 600, train_loss: 0.0013787, test_loss: 0.0070223\n",
      "\tTrain Epoch: 600, train_loss: 0.0007152, test_loss: 0.0009085\n",
      "\tTrain Epoch: 600, train_loss: 0.0015084, test_loss: 0.0031550\n",
      "\tTrain Epoch: 600, train_loss: 0.0008421, test_loss: 0.0008992\n",
      "\tTrain Epoch: 600, train_loss: 0.0018412, test_loss: 0.0024392\n",
      "\tTrain Epoch: 600, train_loss: 0.0027495, test_loss: 0.0046385\n",
      "\tTrain Epoch: 600, train_loss: 0.0018833, test_loss: 0.0023415\n",
      "\tTrain Epoch: 600, train_loss: 0.0020259, test_loss: 0.0025793\n",
      "\tTrain Epoch: 600, train_loss: 0.0015429, test_loss: 0.0020703\n",
      "\tTrain Epoch: 600, train_loss: 0.0021731, test_loss: 0.0017788\n",
      "\tTrain Epoch: 600, train_loss: 0.0008615, test_loss: 0.0013473\n",
      "\tTrain Epoch: 600, train_loss: 0.0012257, test_loss: 0.0015824\n",
      "\tTrain Epoch: 600, train_loss: 0.0020056, test_loss: 0.0024699\n",
      "\tTrain Epoch: 600, train_loss: 0.0006211, test_loss: 0.0010471\n",
      "\tTrain Epoch: 600, train_loss: 0.0015068, test_loss: 0.0014682\n",
      "\tTrain Epoch: 600, train_loss: 0.0002398, test_loss: 0.0004432\n",
      "\tTrain Epoch: 600, train_loss: 0.0014012, test_loss: 0.0015618\n",
      "\tTrain Epoch: 600, train_loss: 0.0009742, test_loss: 0.0018888\n",
      "\tTrain Epoch: 600, train_loss: 0.0003393, test_loss: 0.0006664\n",
      "\tTrain Epoch: 600, train_loss: 0.0008133, test_loss: 0.0028924\n",
      "\tTrain Epoch: 600, train_loss: 0.0004689, test_loss: 0.0020535\n",
      "\tTrain Epoch: 600, train_loss: 0.0008572, test_loss: 0.0022758\n",
      "\tTrain Epoch: 600, train_loss: 0.0002827, test_loss: 0.0004016\n",
      "\tTrain Epoch: 600, train_loss: 0.0007287, test_loss: 0.0039003\n",
      "\tTrain Epoch: 600, train_loss: 0.0004626, test_loss: 0.0007787\n",
      "\tTrain Epoch: 600, train_loss: 0.0013661, test_loss: 0.0027959\n",
      "\tTrain Epoch: 600, train_loss: 0.0007027, test_loss: 0.0025210\n",
      "\tTrain Epoch: 600, train_loss: 0.0013290, test_loss: 0.0014155\n",
      "\tTrain Epoch: 600, train_loss: 0.0008381, test_loss: 0.0011787\n",
      "\tTrain Epoch: 600, train_loss: 0.0008251, test_loss: 0.0011459\n",
      "\tTrain Epoch: 600, train_loss: 0.0003619, test_loss: 0.0009765\n",
      "\tTrain Epoch: 600, train_loss: 0.0016683, test_loss: 0.0065086\n",
      "\tTrain Epoch: 600, train_loss: 0.0009392, test_loss: 0.0050323\n",
      "\tTrain Epoch: 600, train_loss: 0.0017289, test_loss: 0.0030117\n",
      "\tTrain Epoch: 600, train_loss: 0.0003174, test_loss: 0.0005756\n",
      "\tTrain Epoch: 600, train_loss: 0.0006721, test_loss: 0.0011044\n",
      "\tTrain Epoch: 600, train_loss: 0.0004690, test_loss: 0.0008971\n",
      "\tTrain Epoch: 600, train_loss: 0.0000002, test_loss: 0.0000002\n",
      "\tTrain Epoch: 600, train_loss: 0.0000003, test_loss: 0.0000001\n",
      "\tTrain Epoch: 600, train_loss: 0.0010456, test_loss: 0.0018042\n",
      "\tTrain Epoch: 600, train_loss: 0.0006094, test_loss: 0.0014023\n",
      "\tTrain Epoch: 600, train_loss: 0.0009760, test_loss: 0.0015850\n",
      "\tTrain Epoch: 600, train_loss: 0.0004895, test_loss: 0.0008823\n",
      "\tTrain Epoch: 600, train_loss: 0.0002514, test_loss: 0.0003065\n",
      "\tTrain Epoch: 600, train_loss: 0.0026284, test_loss: 0.0048301\n",
      "\tTrain Epoch: 600, train_loss: 0.0004701, test_loss: 0.0005668\n",
      "\tTrain Epoch: 600, train_loss: 0.0015041, test_loss: 0.0019557\n",
      "\tTrain Epoch: 600, train_loss: 0.0007019, test_loss: 0.0010219\n",
      "\tTrain Epoch: 600, train_loss: 0.0008792, test_loss: 0.0017068\n",
      "\tTrain Epoch: 600, train_loss: 0.0012359, test_loss: 0.0022797\n",
      "\tTrain Epoch: 600, train_loss: 0.0009801, test_loss: 0.0016191\n",
      "\tTrain Epoch: 600, train_loss: 0.0005154, test_loss: 0.0005306\n",
      "\tTrain Epoch: 600, train_loss: 0.0013863, test_loss: 0.0021014\n",
      "\tTrain Epoch: 600, train_loss: 0.0004112, test_loss: 0.0004588\n",
      "\tTrain Epoch: 600, train_loss: 0.0010730, test_loss: 0.0024296\n",
      "\tTrain Epoch: 600, train_loss: 0.0004084, test_loss: 0.0004429\n",
      "\tTrain Epoch: 600, train_loss: 0.0014403, test_loss: 0.0022744\n",
      "\tTrain Epoch: 600, train_loss: 0.0004187, test_loss: 0.0007443\n",
      "\tTrain Epoch: 600, train_loss: 0.0014638, test_loss: 0.0021900\n",
      "\tTrain Epoch: 600, train_loss: 0.0008316, test_loss: 0.0012840\n",
      "\tTrain Epoch: 600, train_loss: 0.0016856, test_loss: 0.0021577\n",
      "\tTrain Epoch: 600, train_loss: 0.0010189, test_loss: 0.0014220\n",
      "\tTrain Epoch: 600, train_loss: 0.0024732, test_loss: 0.0034057\n",
      "\tTrain Epoch: 600, train_loss: 0.0004530, test_loss: 0.0005157\n",
      "\tTrain Epoch: 600, train_loss: 0.0020425, test_loss: 0.0029351\n",
      "\tTrain Epoch: 600, train_loss: 0.0012253, test_loss: 0.0017093\n",
      "\tTrain Epoch: 600, train_loss: 0.0017309, test_loss: 0.0037983\n",
      "\tTrain Epoch: 600, train_loss: 0.0022443, test_loss: 0.0036220\n",
      "\tTrain Epoch: 600, train_loss: 0.0016235, test_loss: 0.0022724\n",
      "\tTrain Epoch: 600, train_loss: 0.0016534, test_loss: 0.0023454\n",
      "\tTrain Epoch: 600, train_loss: 0.0009902, test_loss: 0.0011709\n",
      "\tTrain Epoch: 600, train_loss: 0.0019579, test_loss: 0.0029763\n",
      "\tTrain Epoch: 600, train_loss: 0.0013026, test_loss: 0.0024162\n",
      "\tTrain Epoch: 600, train_loss: 0.0012053, test_loss: 0.0016241\n",
      "\tTrain Epoch: 600, train_loss: 0.0022868, test_loss: 0.0041444\n",
      "\tTrain Epoch: 600, train_loss: 0.0008852, test_loss: 0.0012102\n",
      "\tTrain Epoch: 600, train_loss: 0.0004318, test_loss: 0.0006281\n",
      "\tTrain Epoch: 600, train_loss: 0.0022207, test_loss: 0.0035325\n",
      "\tTrain Epoch: 600, train_loss: 0.0019581, test_loss: 0.0032324\n",
      "\tTrain Epoch: 600, train_loss: 0.0004218, test_loss: 0.0015813\n",
      "\tTrain Epoch: 600, train_loss: 0.0002443, test_loss: 0.0002202\n",
      "\tTrain Epoch: 600, train_loss: 0.0005543, test_loss: 0.0013090\n",
      "\tTrain Epoch: 600, train_loss: 0.0002491, test_loss: 0.0006465\n",
      "\tTrain Epoch: 600, train_loss: 0.0003632, test_loss: 0.0031740\n",
      "\tTrain Epoch: 600, train_loss: 0.0006386, test_loss: 0.0011162\n",
      "\tTrain Epoch: 600, train_loss: 0.0011796, test_loss: 0.0015141\n",
      "\tTrain Epoch: 600, train_loss: 0.0007105, test_loss: 0.0011071\n",
      "\tTrain Epoch: 600, train_loss: 0.0031856, test_loss: 0.0035642\n",
      "\tTrain Epoch: 600, train_loss: 0.0019325, test_loss: 0.0025932\n",
      "\tTrain Epoch: 600, train_loss: 0.0005859, test_loss: 0.0007164\n",
      "\tTrain Epoch: 600, train_loss: 0.0011716, test_loss: 0.0018568\n"
     ]
    }
   ],
   "source": [
    "for istation in range(81):\n",
    "    for jinout in range(2):\n",
    "        net = LSTMRG(input_size = train_feature.shape[-1], hidden_size = hidden_size, output_size=1, num_layers=num_layers)\n",
    "        net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "        net.cuda()\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        for epoch in range(EPOCH):\n",
    "            net.train()\n",
    "            out = net(train_feature[istation][jinout])\n",
    "            loss = criterion(out, train_label[istation][jinout])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (epoch + 1) % EPOCH == 0:\n",
    "                net.eval()\n",
    "                pred_label = net(test_feature[istation][jinout])\n",
    "                pred_label = pred_label.data.cpu().numpy()\n",
    "                mse = np.mean((pred_label - test_label[istation][jinout]) ** 2)\n",
    "                print('\\tTrain Epoch: %d, train_loss: %.7f, test_loss: %.7f' % (epoch + 1, loss, mse))\n",
    "        torch.save(net.state_dict(), 'model/station-' + str(istation).zfill(2) + '-' + str(jinout) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
