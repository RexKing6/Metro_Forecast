{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = np.load('train_feature.npy')\n",
    "train_label = np.load('train_label.npy')\n",
    "test_feature = np.load('test_feature.npy')\n",
    "test_label = np.load('test_label.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = torch.from_numpy(train_feature)\n",
    "train_feature = train_feature.float()\n",
    "train_feature = Variable(train_feature)\n",
    "train_feature = train_feature.cuda()\n",
    "\n",
    "train_label = torch.from_numpy(train_label)\n",
    "train_label = train_label.float()\n",
    "train_label = Variable(train_label)\n",
    "train_label = train_label.cuda()\n",
    "\n",
    "test_feature = torch.from_numpy(test_feature)\n",
    "test_feature = test_feature.float()\n",
    "test_feature = Variable(test_feature)\n",
    "test_feature = test_feature.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 18\n",
    "num_layers = 1\n",
    "learning_rate = 0.01\n",
    "EPOCH = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRG(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n",
    "        super(LSTMRG, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, dropout=0.5)\n",
    "        self.reg1 = nn.Linear(hidden_size, int(hidden_size/2))  # 拼接全连接层\n",
    "        self.reg2 = nn.Linear(int(hidden_size/2), output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.rnn(x)\n",
    "        s, b, h = x.shape\n",
    "        x = x.view(s * b, h)\n",
    "        x = self.reg1(x)\n",
    "        x = F.tanh(x)\n",
    "        x = self.reg2(x)\n",
    "        x = x.view(s, b, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Epoch: 600, train_loss: 0.0013558, test_loss: 0.0012953\n",
      "\tTrain Epoch: 600, train_loss: 0.0017885, test_loss: 0.0024750\n",
      "\tTrain Epoch: 600, train_loss: 0.0004533, test_loss: 0.0005581\n",
      "\tTrain Epoch: 600, train_loss: 0.0018720, test_loss: 0.0020371\n",
      "\tTrain Epoch: 600, train_loss: 0.0013014, test_loss: 0.0015776\n",
      "\tTrain Epoch: 600, train_loss: 0.0006063, test_loss: 0.0005727\n",
      "\tTrain Epoch: 600, train_loss: 0.0008405, test_loss: 0.0007503\n",
      "\tTrain Epoch: 600, train_loss: 0.0015026, test_loss: 0.0016506\n",
      "\tTrain Epoch: 600, train_loss: 0.0006950, test_loss: 0.0008599\n",
      "\tTrain Epoch: 600, train_loss: 0.0003697, test_loss: 0.0004642\n",
      "\tTrain Epoch: 600, train_loss: 0.0006845, test_loss: 0.0009691\n",
      "\tTrain Epoch: 600, train_loss: 0.0004831, test_loss: 0.0004383\n",
      "\tTrain Epoch: 600, train_loss: 0.0014928, test_loss: 0.0022309\n",
      "\tTrain Epoch: 600, train_loss: 0.0030422, test_loss: 0.0030780\n",
      "\tTrain Epoch: 600, train_loss: 0.0026175, test_loss: 0.0032024\n",
      "\tTrain Epoch: 600, train_loss: 0.0015766, test_loss: 0.0014872\n",
      "\tTrain Epoch: 600, train_loss: 0.0009784, test_loss: 0.0010114\n",
      "\tTrain Epoch: 600, train_loss: 0.0009793, test_loss: 0.0011312\n",
      "\tTrain Epoch: 600, train_loss: 0.0011260, test_loss: 0.0012088\n",
      "\tTrain Epoch: 600, train_loss: 0.0021789, test_loss: 0.0024131\n",
      "\tTrain Epoch: 600, train_loss: 0.0010212, test_loss: 0.0018699\n",
      "\tTrain Epoch: 600, train_loss: 0.0008044, test_loss: 0.0008983\n",
      "\tTrain Epoch: 600, train_loss: 0.0018182, test_loss: 0.0022501\n",
      "\tTrain Epoch: 600, train_loss: 0.0010343, test_loss: 0.0013134\n",
      "\tTrain Epoch: 600, train_loss: 0.0009028, test_loss: 0.0011115\n",
      "\tTrain Epoch: 600, train_loss: 0.0008124, test_loss: 0.0008831\n",
      "\tTrain Epoch: 600, train_loss: 0.0008478, test_loss: 0.0011748\n",
      "\tTrain Epoch: 600, train_loss: 0.0016214, test_loss: 0.0016389\n",
      "\tTrain Epoch: 600, train_loss: 0.0004396, test_loss: 0.0005013\n",
      "\tTrain Epoch: 600, train_loss: 0.0012577, test_loss: 0.0016610\n",
      "\tTrain Epoch: 600, train_loss: 0.0032580, test_loss: 0.0081434\n",
      "\tTrain Epoch: 600, train_loss: 0.0035023, test_loss: 0.0047365\n",
      "\tTrain Epoch: 600, train_loss: 0.0004737, test_loss: 0.0004105\n",
      "\tTrain Epoch: 600, train_loss: 0.0006583, test_loss: 0.0007835\n",
      "\tTrain Epoch: 600, train_loss: 0.0011782, test_loss: 0.0014499\n",
      "\tTrain Epoch: 600, train_loss: 0.0008258, test_loss: 0.0009261\n",
      "\tTrain Epoch: 600, train_loss: 0.0006816, test_loss: 0.0006707\n",
      "\tTrain Epoch: 600, train_loss: 0.0005662, test_loss: 0.0006784\n",
      "\tTrain Epoch: 600, train_loss: 0.0014813, test_loss: 0.0015696\n",
      "\tTrain Epoch: 600, train_loss: 0.0009903, test_loss: 0.0011045\n",
      "\tTrain Epoch: 600, train_loss: 0.0004877, test_loss: 0.0004302\n",
      "\tTrain Epoch: 600, train_loss: 0.0015770, test_loss: 0.0020591\n",
      "\tTrain Epoch: 600, train_loss: 0.0006235, test_loss: 0.0005564\n",
      "\tTrain Epoch: 600, train_loss: 0.0019170, test_loss: 0.0020402\n",
      "\tTrain Epoch: 600, train_loss: 0.0006181, test_loss: 0.0009499\n",
      "\tTrain Epoch: 600, train_loss: 0.0017488, test_loss: 0.0017429\n",
      "\tTrain Epoch: 600, train_loss: 0.0024247, test_loss: 0.0016501\n",
      "\tTrain Epoch: 600, train_loss: 0.0024011, test_loss: 0.0022191\n",
      "\tTrain Epoch: 600, train_loss: 0.0028642, test_loss: 0.0019254\n",
      "\tTrain Epoch: 600, train_loss: 0.0034849, test_loss: 0.0028609\n",
      "\tTrain Epoch: 600, train_loss: 0.0025083, test_loss: 0.0037483\n",
      "\tTrain Epoch: 600, train_loss: 0.0013734, test_loss: 0.0013155\n",
      "\tTrain Epoch: 600, train_loss: 0.0019492, test_loss: 0.0039425\n",
      "\tTrain Epoch: 600, train_loss: 0.0020925, test_loss: 0.0026410\n",
      "\tTrain Epoch: 600, train_loss: 0.0004891, test_loss: 0.0006419\n",
      "\tTrain Epoch: 600, train_loss: 0.0015849, test_loss: 0.0018481\n",
      "\tTrain Epoch: 600, train_loss: 0.0015545, test_loss: 0.0013669\n",
      "\tTrain Epoch: 600, train_loss: 0.0021384, test_loss: 0.0019325\n",
      "\tTrain Epoch: 600, train_loss: 0.0009285, test_loss: 0.0008204\n",
      "\tTrain Epoch: 600, train_loss: 0.0019329, test_loss: 0.0020557\n",
      "\tTrain Epoch: 600, train_loss: 0.0011111, test_loss: 0.0013614\n",
      "\tTrain Epoch: 600, train_loss: 0.0024821, test_loss: 0.0020048\n",
      "\tTrain Epoch: 600, train_loss: 0.0015332, test_loss: 0.0022023\n",
      "\tTrain Epoch: 600, train_loss: 0.0014616, test_loss: 0.0030741\n",
      "\tTrain Epoch: 600, train_loss: 0.0007136, test_loss: 0.0006892\n",
      "\tTrain Epoch: 600, train_loss: 0.0014766, test_loss: 0.0018607\n",
      "\tTrain Epoch: 600, train_loss: 0.0007011, test_loss: 0.0005920\n",
      "\tTrain Epoch: 600, train_loss: 0.0018324, test_loss: 0.0019849\n",
      "\tTrain Epoch: 600, train_loss: 0.0008450, test_loss: 0.0009408\n",
      "\tTrain Epoch: 600, train_loss: 0.0025335, test_loss: 0.0030280\n",
      "\tTrain Epoch: 600, train_loss: 0.0005782, test_loss: 0.0006792\n",
      "\tTrain Epoch: 600, train_loss: 0.0020991, test_loss: 0.0023293\n",
      "\tTrain Epoch: 600, train_loss: 0.0009170, test_loss: 0.0009837\n",
      "\tTrain Epoch: 600, train_loss: 0.0014192, test_loss: 0.0024360\n",
      "\tTrain Epoch: 600, train_loss: 0.0008946, test_loss: 0.0010908\n",
      "\tTrain Epoch: 600, train_loss: 0.0019127, test_loss: 0.0023903\n",
      "\tTrain Epoch: 600, train_loss: 0.0026791, test_loss: 0.0027494\n",
      "\tTrain Epoch: 600, train_loss: 0.0019281, test_loss: 0.0017736\n",
      "\tTrain Epoch: 600, train_loss: 0.0022385, test_loss: 0.0031267\n",
      "\tTrain Epoch: 600, train_loss: 0.0020330, test_loss: 0.0024903\n",
      "\tTrain Epoch: 600, train_loss: 0.0021185, test_loss: 0.0060982\n",
      "\tTrain Epoch: 600, train_loss: 0.0010539, test_loss: 0.0010518\n",
      "\tTrain Epoch: 600, train_loss: 0.0010925, test_loss: 0.0019748\n",
      "\tTrain Epoch: 600, train_loss: 0.0018695, test_loss: 0.0024726\n",
      "\tTrain Epoch: 600, train_loss: 0.0004694, test_loss: 0.0003839\n",
      "\tTrain Epoch: 600, train_loss: 0.0014414, test_loss: 0.0017850\n",
      "\tTrain Epoch: 600, train_loss: 0.0002781, test_loss: 0.0002559\n",
      "\tTrain Epoch: 600, train_loss: 0.0016759, test_loss: 0.0016347\n",
      "\tTrain Epoch: 600, train_loss: 0.0004974, test_loss: 0.0005200\n",
      "\tTrain Epoch: 600, train_loss: 0.0002375, test_loss: 0.0003718\n",
      "\tTrain Epoch: 600, train_loss: 0.0007360, test_loss: 0.0012813\n",
      "\tTrain Epoch: 600, train_loss: 0.0004765, test_loss: 0.0007674\n",
      "\tTrain Epoch: 600, train_loss: 0.0010454, test_loss: 0.0010741\n",
      "\tTrain Epoch: 600, train_loss: 0.0002818, test_loss: 0.0002501\n",
      "\tTrain Epoch: 600, train_loss: 0.0007548, test_loss: 0.0008412\n",
      "\tTrain Epoch: 600, train_loss: 0.0005426, test_loss: 0.0004611\n",
      "\tTrain Epoch: 600, train_loss: 0.0016427, test_loss: 0.0022220\n",
      "\tTrain Epoch: 600, train_loss: 0.0010056, test_loss: 0.0007710\n",
      "\tTrain Epoch: 600, train_loss: 0.0018199, test_loss: 0.0015682\n",
      "\tTrain Epoch: 600, train_loss: 0.0006883, test_loss: 0.0007557\n",
      "\tTrain Epoch: 600, train_loss: 0.0010293, test_loss: 0.0016018\n",
      "\tTrain Epoch: 600, train_loss: 0.0005139, test_loss: 0.0003655\n",
      "\tTrain Epoch: 600, train_loss: 0.0022173, test_loss: 0.0021483\n",
      "\tTrain Epoch: 600, train_loss: 0.0010584, test_loss: 0.0013078\n",
      "\tTrain Epoch: 600, train_loss: 0.0008162, test_loss: 0.0006274\n",
      "\tTrain Epoch: 600, train_loss: 0.0005191, test_loss: 0.0003061\n",
      "\tTrain Epoch: 600, train_loss: 0.0008585, test_loss: 0.0010710\n",
      "\tTrain Epoch: 600, train_loss: 0.0004843, test_loss: 0.0008770\n",
      "\tTrain Epoch: 600, train_loss: 0.0000010, test_loss: 0.0000010\n",
      "\tTrain Epoch: 600, train_loss: 0.0000010, test_loss: 0.0000010\n",
      "\tTrain Epoch: 600, train_loss: 0.0011108, test_loss: 0.0011532\n",
      "\tTrain Epoch: 600, train_loss: 0.0006547, test_loss: 0.0005847\n",
      "\tTrain Epoch: 600, train_loss: 0.0009511, test_loss: 0.0016492\n",
      "\tTrain Epoch: 600, train_loss: 0.0005819, test_loss: 0.0007117\n",
      "\tTrain Epoch: 600, train_loss: 0.0002620, test_loss: 0.0003365\n",
      "\tTrain Epoch: 600, train_loss: 0.0014049, test_loss: 0.0015679\n",
      "\tTrain Epoch: 600, train_loss: 0.0005819, test_loss: 0.0007692\n",
      "\tTrain Epoch: 600, train_loss: 0.0016895, test_loss: 0.0019636\n",
      "\tTrain Epoch: 600, train_loss: 0.0006889, test_loss: 0.0008460\n",
      "\tTrain Epoch: 600, train_loss: 0.0009360, test_loss: 0.0009804\n",
      "\tTrain Epoch: 600, train_loss: 0.0013978, test_loss: 0.0019534\n",
      "\tTrain Epoch: 600, train_loss: 0.0012082, test_loss: 0.0011459\n",
      "\tTrain Epoch: 600, train_loss: 0.0005200, test_loss: 0.0010580\n",
      "\tTrain Epoch: 600, train_loss: 0.0014398, test_loss: 0.0011480\n",
      "\tTrain Epoch: 600, train_loss: 0.0004634, test_loss: 0.0006571\n",
      "\tTrain Epoch: 600, train_loss: 0.0014376, test_loss: 0.0020673\n",
      "\tTrain Epoch: 600, train_loss: 0.0004797, test_loss: 0.0004243\n",
      "\tTrain Epoch: 600, train_loss: 0.0014748, test_loss: 0.0013847\n",
      "\tTrain Epoch: 600, train_loss: 0.0004435, test_loss: 0.0004177\n",
      "\tTrain Epoch: 600, train_loss: 0.0015970, test_loss: 0.0017781\n",
      "\tTrain Epoch: 600, train_loss: 0.0010659, test_loss: 0.0009508\n",
      "\tTrain Epoch: 600, train_loss: 0.0014569, test_loss: 0.0016418\n",
      "\tTrain Epoch: 600, train_loss: 0.0012764, test_loss: 0.0012937\n",
      "\tTrain Epoch: 600, train_loss: 0.0025752, test_loss: 0.0026688\n",
      "\tTrain Epoch: 600, train_loss: 0.0005019, test_loss: 0.0005512\n",
      "\tTrain Epoch: 600, train_loss: 0.0019605, test_loss: 0.0028735\n",
      "\tTrain Epoch: 600, train_loss: 0.0011834, test_loss: 0.0040721\n",
      "\tTrain Epoch: 600, train_loss: 0.0020605, test_loss: 0.0029837\n",
      "\tTrain Epoch: 600, train_loss: 0.0020741, test_loss: 0.0036373\n",
      "\tTrain Epoch: 600, train_loss: 0.0014306, test_loss: 0.0021697\n",
      "\tTrain Epoch: 600, train_loss: 0.0011964, test_loss: 0.0015401\n",
      "\tTrain Epoch: 600, train_loss: 0.0008970, test_loss: 0.0009044\n",
      "\tTrain Epoch: 600, train_loss: 0.0019681, test_loss: 0.0027325\n",
      "\tTrain Epoch: 600, train_loss: 0.0014149, test_loss: 0.0017512\n",
      "\tTrain Epoch: 600, train_loss: 0.0012172, test_loss: 0.0015198\n",
      "\tTrain Epoch: 600, train_loss: 0.0021686, test_loss: 0.0031418\n",
      "\tTrain Epoch: 600, train_loss: 0.0009161, test_loss: 0.0008947\n",
      "\tTrain Epoch: 600, train_loss: 0.0004054, test_loss: 0.0004042\n",
      "\tTrain Epoch: 600, train_loss: 0.0024278, test_loss: 0.0052300\n",
      "\tTrain Epoch: 600, train_loss: 0.0026842, test_loss: 0.0034270\n",
      "\tTrain Epoch: 600, train_loss: 0.0002943, test_loss: 0.0003234\n",
      "\tTrain Epoch: 600, train_loss: 0.0002509, test_loss: 0.0001917\n",
      "\tTrain Epoch: 600, train_loss: 0.0006347, test_loss: 0.0007208\n",
      "\tTrain Epoch: 600, train_loss: 0.0004124, test_loss: 0.0003393\n",
      "\tTrain Epoch: 600, train_loss: 0.0004709, test_loss: 0.0005294\n",
      "\tTrain Epoch: 600, train_loss: 0.0002899, test_loss: 0.0002567\n",
      "\tTrain Epoch: 600, train_loss: 0.0012597, test_loss: 0.0016565\n",
      "\tTrain Epoch: 600, train_loss: 0.0008911, test_loss: 0.0011420\n",
      "\tTrain Epoch: 600, train_loss: 0.0008328, test_loss: 0.0031186\n",
      "\tTrain Epoch: 600, train_loss: 0.0022553, test_loss: 0.0034568\n",
      "\tTrain Epoch: 600, train_loss: 0.0005936, test_loss: 0.0006410\n",
      "\tTrain Epoch: 600, train_loss: 0.0012833, test_loss: 0.0014022\n"
     ]
    }
   ],
   "source": [
    "for istation in range(81):\n",
    "    for jinout in range(2):\n",
    "        net = LSTMRG(input_size = train_feature.shape[-1], hidden_size = hidden_size, output_size=1, num_layers=num_layers)\n",
    "        net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "        net.cuda()\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "        for epoch in range(EPOCH):\n",
    "            net.train()\n",
    "            out = net(train_feature[istation][jinout])\n",
    "            loss = criterion(out, train_label[istation][jinout])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (epoch + 1) % EPOCH == 0:\n",
    "                net.eval()\n",
    "                pred_label = net(test_feature[istation][jinout])\n",
    "                pred_label = pred_label.data.cpu().numpy()\n",
    "                mse = np.mean((pred_label - test_label[istation][jinout]) ** 2)\n",
    "                print('\\tTrain Epoch: %d, train_loss: %.7f, test_loss: %.7f' % (epoch + 1, loss, mse))\n",
    "        torch.save(net.state_dict(), 'model/station-' + str(istation).zfill(2) + '-' + str(jinout) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
